\documentclass[a4page, 11pt]{article}

\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[margin=0.65in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[autostyle]{csquotes}
\usepackage{enumitem}
\usepackage[backend=biber]{biblatex}

\addbibresource{biblio.bib}


\graphicspath{ {./img/} }

\title{Data Management}
\author{}
\date{}
%NEED TO IMPROVE SECTION 6 DATA DISTRIBUTION
\begin{document}

\maketitle

%%% SOLO QUESTO E' ITALIANO %%%
\section{RELATIONAL MODEL}

Aspetti positivi :

\begin{enumerate}[noitemsep]
	 
	\item
	Molto rigido come regole.
	\item
	Il RDBMS sfrutta le proprietà ACID :
	\begin{itemize}
		
		\item
		A : Atomicity : serve affinché l'operazione o avviene su tutti i dati o non avviene, ad esempio se vi è un aggiornamento nei dati questo aiuta affinché i dati non vengano aggiornati solo parzialmente.
		\item
		C : Consistency : I dati o il nuovo dato che viene aggiunto rispetta lo schema prestabilito. E se si viola la consistenza con un'operazione tutta l'operazione fallisce.
		\item
		I : Isolation : gestisce come l'integrità delle transazioni sono viste dall'utente. E garantisce che durante un'operazione (query) non vengano svolte altre operazioni. E che lo stato del database venga modifica solo prima della fine della query.
		\item
		D : Durability : garantisce che una transazioni effettuata sopravvivrà per sempre. Anche se il sistema crasha dovuto grazie ai server di
		backup e log files.
	\end{itemize}
\end{enumerate}




Il RDBMS esiste già da 35 anni quindi è ben sviluppata e molto conosciuto, molti dati sono ancora salvati in questo formato ed è efficace ancora per molte operazioni. Gli aspetti limitanti sono:

\begin{enumerate}[noitemsep]
	 
	\item
	un attributo può avere solo un valore.
	\item
	non è compatibile con molti linguaggi moderni.
	\item
	molto rigido come linguaggio
	\item
	non accetta i loop.
	\item
	Per i RDBMS :
	\begin{itemize}
		
		\item
		Difficile modificare le tabelle.
		\item
		La performance,
	\end{itemize}
\end{enumerate}

Nei RDBMS la performance (inteso come velocità) dipende da vari fattori:
\begin{itemize}[noitemsep]
	 
	\item
	Numero delle righe
	\item
	Tipo di operazione
	\item
	Algoritmo scelto
	\item
	La struttura dati scelta
\end{itemize}

Per Scaling Up intendiamo potenziare le macchine, mentre per Scaling out intendiamo aggiungere macchine, per i RDBMS è più facile Scale up che Scale out. 
Ulteriormente il costo è un altro dei problemi, installare il software richiede un costo molto alto e hardware molto complesso.
Inoltre se continuiamo a aggiungere server (scaling out) il prezzo dell'Hardware aumenta esponenzialmente mentre il tempo di risposta scende asintoticamente.

Di fronte a questi svantaggi sorge una nuova ``tecnologia'' i NoSQL(Not Only SQL), le proprietà di NoSQL sono:

\begin{enumerate}[noitemsep]
	 
	\item
	Non ha nessun schema o modello prefissato
	\begin{itemize}
		
		\item
		A differenza dei modelli SQL nei quali bisogna prima definire il modello, qua nei NoSQL non esiste un modello rigido.
		\item
		Per aggiungere un nuovo attributo non vi è bisogno di cambiare il modello a differenza dei SQL
		\item
		I modelli NoSQL seguono l'assunzione del mondo aperto (ciò che non è vero è sconosciuto(ma non per forza falso)) mentre SQL segue l'assunzione del mondo chiuso (solo ciò che è noto come vero è vero)
	\end{itemize}

	\def\labelenumi{\arabic{enumi}.}
	 
	\item
	Segue il teorema CAP:
	\begin{itemize}
		\item E' impossibile per un sistema informatico distribuito(vuol dire sistemi interconnessi tra loro e la comunicazione avviene solo attraverso messaggi) fornire simultaneamente le tre garanzie(Infatti può soddisfare solo 2 di esse):
		\begin{enumerate}		
			\item
			\textbf{(C)C}oerenza : Tutti i nodi vedano gli stessi dati allo stesso istante. Se è assente allora una soluzione è mostrare il dato precedente alla modifica cioè non quello più recente.
			\item
			\textbf{(A)}vailability(Disponibilità) : La garanzia che ogni richiesta ottenga una risposta su ciò che è fallito e ciò che ha avuto successo. Se è assente aspetterò per lunghi tempi senza ricevere una risposta.
			\item
			\textbf{(P)}artition Tolerance(Tolleranza sulle partizioni) : Il sistema funziona anche dopo aver perso un numero arbitrario di pezzi del sistema.
		\end{enumerate}
		\item
		I sistemi RDBMS sono dei software CA, ed è possibile creare dei modelli RDBMS basati sul CAP
		\item
		I sistemi NoSQL sono dei sistemi solitamente CP o AP.
		\item
		Uno preferisce la disponibilità sulla coerenza, perché è meglio vedere un vecchio dato non coerente che vedere un errore di fallimento di caricamento dei dati.
	\end{itemize}

	\item
	Segue Principio BASE:
	\begin{itemize}
		
		\item
		Basic Availability : completare una richiesta anche se parzialmente consistente, ad esempio nel caso di fallimento. E' possibile fare ciò grazie al fatto che usa server sparsi ovunque con un grado di	replicazione del database e in caso di malfunzionamento del database richiesto non tutto il sistema cede la Disponibilità.
		\item
		Soft State : Abbandonano la richiesta di consistenza dei ACID praticamente completamente, cioè i dati possono avere schemi diversi.
		\item
		Eventual consistency : I sistemi NoSQL richiedono che a un certo punto i dati convergeranno a uno stato consistente (non si fa garanzie sul quando), e quindi prima di allora ho una consistenza ritardata cioè prima del momenti dello stato consistente posso ricevere come risposta qualunque valore come risposta di una query.
	\end{itemize}
\end{enumerate}



\begin{center}
\begin{tabular}{|l|l|}
\hline
ACID & BASE \\
\hline
• Forte Coerenza & • Coerenza debole\\
• Poca disponibilità & • Disponibilità è la cosa principale\\
• Pessimo “Multitasking” & \quad e sacrifica per questo (CAP)\\
• Complesso & • Veloce e Semplice\\
\hline

\end{tabular}
\end{center}

Esempi e tipologie di modelli NoSQL :

\begin{enumerate}[noitemsep]
	 
	\item
	Key Value (Dynamo, Voldemort, Rhino DHT) : Sono delle tabelle con	chiavi che si riferiscono/puntano a un certo dato, è molto simile a Document based. 
	\item
	Column family (Big Table, Cassandra) : In grado di salvare grandi quantità di dati, la chiave colonna si riferisce a un certo dato ragruppato in collona.
	\item
	Document based (CouchDB, MongoDB) : Di solito salvati con file JSON, salvati come una \textless{}chiave -- valore\textgreater{}. E' facile ricercare dati in questo formato. JSON è basato su due strutture :1) chiave -- valore per gli oggetti e 2) lista ordinata di elementi.
	\item
	Graph based (Neo4J, FlockDB) : Uso i vertici/nodi e archi per
	rappresentare i dati e legami tra di loro. E' difficile fare scaling	con i grafi.
\end{enumerate}

Dobbiamo sacrificare o la dimensione del database o la sua non complessità:
\begin{center}
	\includegraphics[scale=0.5]{IMAGE1.jpg}
\end{center}



\section{MongoDB\cite{MongoDB}}
MongoDB is as already mentioned a document based management system, with the data stored in Bson (Binary Json), and the access to data is possible thanks to indexes. Compared to the SQL DBs, Mongo doesn't have the join feature. The name changes are:

\begin{center}
	\includegraphics[scale=0.6]{IMAGE2.jpg}
\end{center}

In case of a massive data upload one might decide to do a few changes in the process to fasten  it up:

\begin{itemize}[noitemsep]
	 
	\item
	Disable the acknowledgment(ack) of the data, which is a signal passed between communicating processes, computers or devices, to signify acknowledgment or receipt of message, as part of a communications
	protocol.
	\item
	Disable the writing on a log file
\end{itemize}

One must be attentive when doing so because any loss will not be registered and lost forever.

For large data-sets it is useful to use some additional structures called indexes, they are similar to the book indexes and act as a faster way to retrieve information, they might require more time during the insertion but makes the queries faster later, one primary index (basic one) is always the defined for the id. How does it exactly works?
Basically without an index Mongo will perform a simple table scan (like SQL) in which it has to look through all the ``book'' to find a query result, The analogy with the book is if we don't have the index for our books we will have to read it whole until finding the point where we wanted to be. 
Indexing avoid this problem (more problematic if the database is large), it is an ordered list that points it's content.
Since indexing slows down the modifications one must choose just a couple of indexes for any given collection, the tricky part is to decide which one. MongoDB gives, by default, a limit of 64 indexes!

The aggregation uses pipeline and options. 
The aggregation pipeline starts processing the documents of the collection and passes the result to the next pipeline in order to get result for example: \$match and \$group. 
We can use the same operator in different pipelines.

Mongo for many reasons (mainly commercial) offers also a SQL interface, we need the connector BI (Business Intelligence) it generates the relational schema and use such schema to access the data.

\section{GraphDB\cite{GraphDB}}

A graph is a collection of nodes and edges which represents their relations. 
It has a lot of applications such as: social media, recommendations, Geo, Logistics network, financial transaction graphs (for fraud detection), master data management, Bioinformatics, authorization and access control.

The labeled property graph model has the following characteristics:

\begin{itemize}[noitemsep]
	 
	\item
	It contains nodes and relationships;
	\item
	Nodes contains properties (key : value pairs);
	\item
	Nodes can be labeled with one or many labels;
	\item
	Relationships are named and directed, with a start and end node;
	\item
	Relationships can contain properties like nodes.
	\item
	%magari migliorare questa parte
	The relationship can be fine-grained or generic: for example in case of Address we can choose distinct relationship like HOME\_ADDRESS or WORK\_ADDRESS or DELIVERY\_ADDRESS which is the fine-grained relationship or we could choose only address and specify in it which kind of address it is ADDRESS\{type : `home'\} or the other ones, this method is called generic relationship. Usually the generic one is preferred,  especially in cases where I need to find all the address of a client, all I need to do is find the ADDRESS relationship, in the fine-grained I need to find one by one all kind of addresses, just	imagine if we had 100 types of addresses. On the other hand to find the DELIVERY\_ADDRESS all I need to do is find ADDRESS\{type:`delivery'\} 
\end{itemize}

A Graph Database (GD) can use either the native or non-native storage and processing engine.

Native Graph Storage is the one optimized for the native graph management \& the non native graph storage stores the data in non graph based model but this model supports a graph query language, examples: Relational, Object oriented DB, Wide Column, in a relational for example in a join bomb we can use a graph to connect two tables. But the problem
with the relational DBs and most of NoSQL DBs is that it lacks relationships.
Moreover in SQL joining tables adds more complexity, and in case of sparse table with null-able column require special checking in the code, in the case of a market, just to see what a customer bought we need to do a lot of expensive joins or the customers that buy a specific product with other products for the recommendation systems. 
For the NoSQL DBs whether key-value, document or column-oriented, we might use the aggregation technique to see the relationship, but the relationship between aggregates aren't citizen in the data model and is costly operation since it's doesn't use index free adjacency(the data is physically near) and since they stay inside of aggregates with structure in form of nested maps and even after the aggregation there is no back link to point backward and run
other interesting queries.

In a native graph storage the attributes and nodes and the referenced nodes are stored together, to optimize the graph processing engine. 
When we perform a query the graph model does not depend on the total number of nodes instead it remain nearly constant because it works locally to the portion of the graph which is connected to the base node, while the other SQL and NoSQL models will suffer in performance speed with the
increase of data. Moreover we can add more data/nodes and relationship without disturbing the already existing model.

The processing engine uses index-free adjacency, meaning that connected nodes physically points to each other in the DB, this makes sure that the retrieval is faster but it comes at a cost: the efficiency of queries that do not use graph traversal, for example the writing time etc.

It's important to note that it's neither good nor bad to use native or non-native engine, simply one needs to choose one based on his/her needs, for example my DB is based on a non-graph backend (like MySQL) so it would be useful to use a non native graph storage.

The properties of the cypher language is :

\begin{enumerate}[noitemsep]
	 
	\item
	Pattern-matching Query language
	\item
	Humane language, easy to learn, read and understand
	\item
	Expressive yet compact
	\item
	Declarative : Say what you want, not how
	\item
	Borrows from well known query languages especially SQL, yet clearly it's different enough to see that it deals with graphs and not SQL	DBs.
	\item
	Aggregation, Ordering, Limit
	\item
	Update the Graph
\end{enumerate}

A real example is the following: We want to manage a server farm, we define a relational model for managing it. 
We know that a user access to application which runs on a VM and each application uses a DB and a secondary DB. 
Each of the VM is hosted on a server which are placed in a rack structure which is managed by a load balancer.

The initial stage of modeling is similar in any kind of DB, we seek to understand and agree on the entities in the domain and how to interrelate, usually done on whiteboard with a diagram which is a graph.
Next stage we seek a E-R (Entity-Relationship) diagram, which is another graph. 
After having a suitable logical model we map it into tables and relations. 
We keep are data in a relational DB, with it's rigid schema.
But keeping the data normalized slows down the query time so we need to denormalize it, because the user's data model must suit the database engine not the user. 
Denormalizing we involve duplicate data to gain query performance, denormalizing is not a trivial task and we accept that there may be substantial data redundancy. 
The problems doesn't stop here, because once created if we need to modify it, which we will need to do in order to match the changes in the production environment, so we will need to do this work all again!

It's better in there cases to directly use the graph DBs, it will avoid the data redundancy and it can adapt really fast in case of a change in the DB.

Another graph traversal language is Gremlin, part of the Apache TinkerPop framework. In this domain specific language (DSL) expressions specify a concatenation of traversal steps, so you basically explain to gremlin step by step what to do.

\section{Key-Value Model}

It's the most simple and flexible model of the NoSQL family, where every key is assigned to a value, it is possible to assign a type to a value.
The values are not query-able, such as a BLOB, where a BLOB(Binary Large OBject) is a collection of binary data stored as a single entity in a DB, for example images, videos etc.:
\begin{center}
\includegraphics[scale=0.5]{IMAGE3.jpg}
\end{center}
An example is the amazon's cart system which uses DynamoDB, a key-value system.

~\\
The basic operation in a key-valued models are:

\begin{itemize}[noitemsep]
	\item
	Insert a new key-value pair
	\item
	Delete a new key-value pair
	\item
	Update a new key-value pair
	\item
	Find a value given the key
\end{itemize}

There is no schema and the values of the data is opaque. The values can be accessed only through the key, and stored values can be anything : numbers, string, JSON, XML, images, binaries etc.
An example of key-value model is Riak and has the following terminology:
%Quanto è figo minipage, l'ho scoperto dopo aver cercato come unire una tabella con un immagine xD!
\begin{center}
	\begin{minipage}[b]{0.4\textwidth}
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Relational} & \textbf{Riak}\\ \hline
		database instance & Riak cluster\\ \hline
		table & bucket \\ \hline
		row & key-value\\ \hline
		rowid & key\\ \hline
	\end{tabular}
	\end{minipage}
	\begin{minipage}{.5\textwidth}\centering
	\includegraphics[scale=0.6]{IMAGE4.jpg}
	\end{minipage}

\end{center}
%Things not to remeber putting just for completation
Some other examples are: Redis, Memcached, Riak, Hazelcast, Encache.

Thanks to the Hash based index, the key-value systems can scale out in a very efficient way. 
The Hash is a mathematical function that assign to a given key it's value, usually we have h(x) = value, usually it returns a pointer to where the data is stored not exactly the data itself, for example h(x) = (x modulo y) where y is the max length of hash table. 
There might a problem with the conflicts but they can be managed. 
Hashing also enables items in the DB to be retrieved quickly. 
The hash table can be easily distributed in a network, it is managed in pile so we can have a key(saved in the pile) x saved in a server and it's succ(x) stored in a different server, thus scaling out really fast. 
The key value terminology is the following:

In a DHT(Distributed Hash Table) it is pretty simple to insert a key-value (k1,v1) basically take the key as input and route messages to the node holding that key and store the value there, and to retrieve the value of k1 it simply finds the node with the key k1 and return it's value v1 from it.

\section{Wide Column and BigTable}
These two models are an evolution, in a certain way, of the key-value models, when we start giving a structure to the value it becomes more complicated so we use wide column(Big Table): The first model was introduced by Google(HBase) and later on by Facebook (Cassandra) some think that Cassandra is still a key-value model and not a wide column.

BigTable is a multidimensional map, which can be accessed by row key, column key and a timestamp. It is sorted, persistent and sparse.
We will consider the \textbf{HBase BigTable} Model.
The data is organized in tables, each table(the tables are multi-versioned) is composed by the column-families that include columns, the cells within a column family are sorted physically and are usually very sparse with most of the cell having NULL value so we can have different rows with different sets of columns. 
BigTable is characterized by the row key, column key, timestamp, the row has the keys, column contains the data and contents. 
The column is divided in families. The timestamp support the multi-version of modification to check how the data changed over time and still be able to access the latest one without any confusion. We can represent it as follows:

\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{row key} & \textbf{column family 1} & \textbf{column family 2} \\
		\hline
		&
		\begin{tabular}{c|c}
			\textbf{column 1} & \textbf{column 2}
		\end{tabular}
		&
		\begin{tabular}{p{1.8cm}|p{1.8cm}|p{1.8cm}}
			\textbf{column 1} & \textbf{column 2} & \textbf{column 3}
		\end{tabular}
		\\
		\hline
		&
		\begin{tabular}{c|c}
		\textit{cell(data)} & \textit{cell(data)}
		\end{tabular}
		& 
		\begin{tabular}{p{1.8cm}|p{1.8cm}|p{1.8cm}}
		\textit{cell(data)}&\textit{cell(data)} &\textit{cell(data)}
		\end{tabular}\\
		\hline
		&
		\begin{tabular}{c|c}
			\textit{cell(data)} & \textit{cell(data)}
		\end{tabular}
		& 
		\begin{tabular}{p{1.8cm}|p{1.8cm}|p{1.8cm}}
			\textit{cell(data)}&\textit{cell(data)} &\textit{cell(data)}
		\end{tabular}\\
		\hline
	\end{tabular}\\
	~\\
	Each row can have different timestamp, so we can have more versions of this table.
\end{center}

The cells within the data (also the key) is without type since it is saved in bytes. The columns are dynamic. So to get a data given a key we need to transform our data in Bytes with a comand .toBytes(``Key''), we can also use python to modify the columns via APIs. This model can be useful for *-To-Many mappings.
The row key is an array of bytes and serves as a primary key for the table.
Each Column Family has a name and contains one or more related columns, the columns can belong to one column family only and is included inside the row with familyName:columnName followed the value, for example we have: \\ 
%cavolo che sbatti i quote i LaTeX, se trovate un modo carino ditemelo
row key $|$ info:\{`height':`170cm', `state':`NY'\}  roles:\{`ASF':`Director', `Hadoop':`Founder'\}$|$\\
The version number of each row is unique within the row key, by default it uses the system's timestamp but they can be user supplied.

Best time to use HBase is when one need to scale out, need random write and/or read, when we need to do thousand of operations per second on multiple TB of Data, when we need to know all the modification done on
the data, when we need a well known and simple access. One can combine a GraphDB with HBase, one example is JanusGraph.
HBase does not support joins, but this can be done in application layer using Scan() and Get() operations of HBase.

\textbf{Cassandra\cite{Cassandra}}: It started as a support for the Facebook inbox search, it was open sourced in 2008 by Facebook and then it became a top-level project under Apache in 2010. The data model is the same as HBase, the only difference is the way it is stored (storage model) and the program algorithms. We can say it is a restricted BigTable, we can have only one column family. It uses a C* language very similar to SQL language.

Cassandra is a column oriented NoSQL system. 
The KeySpace is the outermost container, it's basic attributes are the Replication Factor (how many copies of the data we need), Replication Strategy and the Column Families.
The Column Families, can be seen as a collection of rows or better a collection of key-value pairs as rows. 
A row is a collection of columns labeled with a name, the value of a row is itself a sequence of key-value pairs where the keys are the column's name and we need at least one column in a row. The columns have the key(name), the value and the timestamp. So a table is a list of “nested key-value pairs”: (ROW x COLUMN key x COLUMN value) and this is inside a column family.
The Key Space is only a logical grouping of columns families.

\begin{center}
	\includegraphics[scale=0.5]{IMAGE5.png}\\
	An example of Cassandra data model.
\end{center}

While with RDBMS, we can have that JOIN of normalized tables can return almost anything, with C* the data model is designed for specific queries and the schema is adjusted as new queries are introduced. In C* there are no JOINS, relationship or foreign keys and the data required by multiple tables are denormalized across those tables.

The Comparison between Apache Cassandra, Google Big Table and Amazon DynamoDB is:
\begin{center}
	\begin{tabular}{|p{3.6cm}|p{3.6cm}|p{3.3cm}|p{4cm}|}
		\hline
		&\textbf{Apache Cassandra} & \textbf{Google Big Table} & \textbf{Amazon DynamoDB}\\
		\hline
		\textit{Storage Type} & Column & Column & Key-Value\\
		\hline
		\textit{Best Use} & Write often\newline read less & Designed for\newline large scalability & Large database solution\\
		\hline
		\textit{Concurrency Control} & MVCC & Locks & ACID\\
		\hline
		\textit{Characteristics} & High Availability\newline Partition\newline Tolerance\newline Persistence & Consistency \newline High Availability\newline Partition\newline Tolerance \newline Persistence & Consistency \newline High Availability\\
		\hline
	\end{tabular}
\end{center}

\section{Data Distribution}
\subsection{Fragmentation and Replication}
A centralized DB system is a system where the database is located, stored and maintained in a single location. If the processor fail all the system fails.
If we need data periodically in different location we can distribute it or replicate it. The distribution is to divide the data and have it stored in different places, it provides opportunities for parallel execution. On the other hand we can also replicate it or replicate part of data in different location improving also the availability.

In a Homogeneous Distributed Database all sites have identical software and aware of each other and agree to cooperate in processing user requests. They surrender their autonomy to change the schema appearing to user as a single system.

In a Heterogeneous Distributed Database different sites may use different schema and software which creates a major problem for query and transaction processing. The sites may not be aware of each other and may provide only limited facilities for cooperation in transaction processing.

The challenges of a distributed database design is deciding what goes, which depends on the data access pattern, and the problem or where to allocate fragments to nodes.
We have that with replication we have that system maintains multiple copies of data, stored in different sites, for faster retrieval and fault tolerance and with the fragmentation the data is divided into several fragments stored in different sites. We can combine both of them so the data is partitioned into several fragments and the system maintains several identical replicas of such fragment.

Dividing/fragmenting our database into n databases, the bottom-lake phenomena arises, in which the slowest machine becomes the bottom-lake: the maximum time for a query is dependent on that machine.

The advantages of replication are:
\begin{itemize}[noitemsep]
	\item
	Availability: Since the failure of a site does not result in unavailability of it's data since it's replicas exist elsewhere.
	\item
	Parallelism: Queries may be processed by several nodes in parallel thus faster.
	\item
	Reduced data transfer: Since replicas of the data may exist in the node itself.
\end{itemize}

While the disadvantages are:

\begin{itemize}[noitemsep]
	 
	\item
	increased cost of updates: Since each replica must be updated.
	\item
	increased complexity of concurrency control e.g. two people book a ticket at the same time solution. We may have that distinct replicas may lead to inconsistent data unless some special concurrency mechanisms are implemented. A solution could be to choose one copy as the primary copy and apply concurrency control operation only on the primary copy.
\end{itemize}

Now for simplicity imagine the database to be relational. 
The fragmentation can be horizontal or vertical, but it must contain sufficient information to reconstruct the original "relation". In the horizontal each tuple of the data is assigned to one or more fragments while in the vertical the schema (in the case of relational) is split into several smaller schemas, and all of them must contain a common candidate key to ensure lossless join for example using a tuple-id attribute.
The advantages are:
\begin{itemize}[noitemsep]
	\item Horizontal:
	\begin{itemize}[noitemsep]
		\item Allows parallel processing on fragments.
		\item Allows the data to be split so the tuples are located where they are more frequently accessed.
	\end{itemize}
	\item Vertical
	\begin{itemize}[noitemsep]
		\item Allows tuples to be split so that each part of tuples are stored where they are more accessed.
		\item tuple-id allows efficient joining of vertical fragments.
		\item Allows parallel processing on a relation.
	\end{itemize}
\end{itemize}
The two fragmentation can be mixed together and the fragments may be successively fragmented to an arbitrary depth. In the centralized system the primary criterion for the cost is the number of disk accesses, in the distributed system we must take in account also the cost of data transmission over the network and the potential gain in performance from having parallel query execution.

Data Transparency is very important it indicates the degree to which the users are not aware on the way the data is stored in a  distributed system.

Summarizing we have that the advantages of DDBMSs are: It reflects the organizational structure, improves share-ability and local autonomy, improves availability, reliability and performance, reduces the hardware cost and have a modular growth.
The disadvantages are: Architecture and database design is more complex, cost, Security, Integrity control is more difficult, there is a lack of standards and experience.

There are 3 types of architecture:
\begin{itemize}
\item Shared Everything: Dominated the architecture market until 2000(approximately), we have one big complex costly data base, it shares everything and can be very slow. It is the centralized system.

\item Share Disk: We have data saved on different disks connected between them, no one uses it. The disks are all connected together and communicates to all the CPUs.

\item Share Nothing: Every thing is separated every disk has it's own CPU and the disks does not communicate with other CPUs or disks, it is very easy to scale it out. Model adopted by NoSQL models. Only at the end the processors are connected together to combine the results from each of them. 
\end{itemize}

\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Shared Disk} & \textbf{Shared Nothing}\\
		\hline
		Quick adaptability to changing workloads & Can exploit simpler, cheaper hardware\\
		\hline
		High availability & Almost unlimited scalability\\
		\hline
		Performs best in a heavy read environment & Works well in high-volume, read/write environment\\
		\hline
		Data need not to be partitioned & Data is partitioned across the cluster\\
		\hline
	\end{tabular}
\end{center}


What to choose between Scalability or Availability? It depends on how much one wants to spend more availability means more money. For example a 100\% availability mean no downtime, decreasing the availability increases the downtime we can have, a 99\% availability means a downtime of 9 - 88 hours.

\textbf{Replication}:

A log is a sequential file that is stored in a stable(never failing) memory, it stores all activities realized by all transaction in a chronological order. The two types of records are the transaction logs (operations) and the system events (Checkpoint \& Dump).

The replicas structures are:
\begin{itemize}[noitemsep]
	\item One2Many: One source and many targets.
	\item Many2One: Many sources and only one target.
	\item Peer2Peer: Data is replicated across multiple nodes that communicates with each other.
	\item Bi-directional: Conceptually a Peer2Peer with only two Peers.
	\item Multi-Tier Staging: The replication is divided in more stages.
\end{itemize}

To create a replica we can:
\begin{itemize}[noitemsep]
	\item take the data form master either the backup copy or the data itself	(if we can stop the activity of the server) and move it. (It could be useful if the data is not accessible from outside)
	\item use log file : to maintain the replica the same as the source even during the transfer (near real time) using the Log to execute the same commands executed on the source.
\end{itemize}


%If you use Volume in project be attentive to the scalability!!!!!
\subsection{MongoDB's Approach: Sharding\cite{ScalingMongoDB}}

MongoDB uses Sharding to split a large collection across several servers(called a cluster): Every document has a key in mongodb so we define a shard key which defines the range of data(chunk range). The key space is like points on a line and the range is a segment of that line.
Example: Key is surname so we can decide all the surnames staring from A to K save in first partition and the others in the second.
MongoDB does the sharding automatically once we tell it to distribute the data. 
The Default maximum chunk size is 64Mb, as a chunks gets bigger, MongoDB will automatically split it into two smaller chunks and if the shards become unbalanced, some chunks will be migrated to other shards with less chunks correct the imbalance. 
The goal of balancer is not only to keep the data evenly distributed but also to minimize the amount of data transferred. 
The balancer is lazy so it won't activate itself until the data is very imbalanced, it does so to avoid moving data back and forth, if it balanced every tiny difference it would just waste resources.

When you first create a shard, MongoDB creates a single chunk with range $(-\infty, +\infty)$ where with $-(+)\infty$ we mean the smallest(largest) value MongoDB can represent. 
From here if the chunk size is bigger that the chosen size it will split and every chunk range must be distinct and not overlapping and their union should cover all the initial range.

The shard is basically a server so we need to use mongos (s=server) to make a query on a shard. mongos is the point interaction between users and the cluster, mongos lets you treat a cluster as a single server. 
The queries are routed to the appropriate shard thanks to mongos. For the not targeted queries (with sorting) a request is sent to all shards and the queries are performed locally and then after the result has returned mongos merges the (sorted) result and returns results to client.
Mongos doesn't actually store any data, the configuration of a cluster is held on a special mongods called config servers, they hold the definitive information about the clusters for everyone's access.

The cluster consists basically of three types of processes:
\begin{itemize}[noitemsep]
	\item The shards: a node of the cluster, it can be either a single monogod or a replica set. 
	MongoDB also automatically replicates the data and in one shard I will have a primary chunk where I store the data I wanted and secondary chunks where MongoDB replicated other data as backup just in case, the secondary is a read only data so I cannot do update queries on it.
	\item Mongos processes: for routing requesting to the correct data and acting as a balancer. It contains no local data and we can have 1 or many of it. After it finishes balancing it must also update the Config Servers with the new positions and while one mongos is balancing the chunks it takes out a "balancer lock" so no other mongos will start the balancing. The lock is released only after the chunk has been copied and the old chunk is deleted from the initial shard.
	\item Config servers: for keeping tracks of the cluster's state. It stores the cluster chunk ranges and locations.
	We can have only 1 or 3.
\end{itemize}

\begin{center}
	\includegraphics[scale=0.50]{IMAGE6.png}
\end{center}


Let's see the steps for the configuration:

\begin{enumerate}[noitemsep]
	\item Start a config server, it start by default at port 27019:

	mongod --configsvr
	\item Start the mongos Router, we need to initialize it like this even if the cluster is already running:
	
	For 1 configuration server: 
	mongos --configdb $<$hostname$>$:27019
	
	For 3 configuration servers:
	mongos --configdb $<$host1$>$:$<$port1$>$,$<$host2$>$:$<$port2$>$,$<$host3$>$:$<$port3$>$
	
	We can have different mongos on same pc on different ports.
	\item Start the shard database, it starts a mongod with the default shard port 27018
	
	mongod --shardsvr
	
	The shard is not yes connected to the rest of the cluster and it may have been already running in production.
	\item Add the Shard:
	
	On mongos: 
	sh.addShard('$<$host$>$:27018')
	
	To add a replica set: 
	sh.addShard('$<$rsname$>$/$<$seedlist$>$')
	\item Verify that the shard was added:
	
	db.runCommand(\{ listshards:1 \}) \newline
	Obtaining as result: \newline
	\{\newline
	"shards": \newline
	[\{ "\_id":"shard0000", "host":"$<$hostname$>$:27018"\}], \newline
	"ok" : 1 \newline
	\}
\end{enumerate} 
To enable sharding on a database simply use the command: sh.enableSharding("$<$dbname$>$") 
\newline To start a collection with the given key:
sh.shardCollection("$<$dbname$>.$people",\{"country" : 1\})

The properties of the Shard key are:
\begin{itemize}[noitemsep]
	\item Shard key is immutable
	\item Shard key values are immutable
	\item Shard key must be indexed
	\item Shard key are limited to 512 bytes in size
	\item Shard key are used to route queries: so choose a field commonly used in queries for the shard key
	\item Only the shard key can be unique across shards, the `\_id' field is only unique within the individual shard
\end{itemize}

%%%%%%%%%%%%%
%%%%%Gibberish
%%%%%%%%%%%

\subsection{HBase}
HBase each tool has a different terminology.

Hbase components is a subset of table's row (horizonal range partiton), which is automatcally done.

Hbase lives bcoz of ZooKeeper.

Hbase is similar to mongodb and have the same flaw : If the master is broken the client cannot obtain any data and we have an error.

Cassandra on the other hand repairs this flaw since there is no master and all the nodes are considered the same peer to peer.

In cassandra since I can add or remove nodes with no downtime we have a transparent elasticity and scalability and since it's P2P architecture we obtain also the High Availavility. 
Also cassandra uses ZooKeeper to find the replicas in other databases, given a node we decide the N number of following nodes in which the replica will we saved.

The data writing is separated in 3 stages : the first one is the log file writing which is essential, even if the data writing fails it doesn't matter bcoz we have the log file. 
And the consistency is based
on majority : if a certain number,which we decide, of the nodes agree that a certain data was written or read it is assumed to be true. 
Or use quorum which states the 50\% +1 nodes agree that something is written it's assumed true.

To delete the data is it easier to consider the data not available(like how recycle bin in OS), and follow it by compactation which reclaims the not available space.


\printbibliography
\end{document}
