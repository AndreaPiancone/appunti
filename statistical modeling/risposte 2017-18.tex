\documentclass[a4page, 11pt]{article}
\usepackage[italian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.3in]{geometry}

\usepackage{amsthm}
\newtheorem*{remark}{Osservazione}

\title{Statistical Modeling}
\author{}
\date{}



\begin{document}
\maketitle
\section{Errori eteroschedastici}
Affinché il nostro modello lineare classico ottenga delle stime efficienti abbiamo bisogno di verificare che gli errori siano omoschedastici. 
In virtù del fatto che ogni errore $\varepsilon$ è a media nulla $E(\varepsilon) = 0$ vale la relazione $E(\varepsilon^2) = Var(\varepsilon) = \sigma^2$, e quindi gli errori sono detti omoschedastici quando la loro varianza è costante al variare del valore dei regressori.
Se ciò non accade gli errori si dicono eteroschedastici $Var(\varepsilon) = \sigma_i^2$; questo incide sulle proprietà degli stimatori OLS: in particolare continuano a valere correttezza e consistenza ma viene meno l’efficienza (lo stimatore non è più \textbf{BLUE}, \textbf{B}est \textbf{L}inear \textbf{U}nbiased \textbf{E}stimator). 
Inoltre le stime campionarie tendono a sottostimare il vero valore della varianza e non esiste più un’unica varianza, ma ce ne sono molteplici. 
Come conseguenza la statistica T di Student ha valori erroneamente elevati ed anche i relativi intervalli di confidenza risulteranno più stretti mentre la regione di rifiuto del test T risulterà erroneamente più ampia; verranno quindi ritenuti significativi i parametri anche quando in realtà non lo sono. 
Per individuare questa caratteristica che ci porta ad un’inaffidabilità delle stime, possiamo ricorrere a diversi metodi (grafici o analitici).

Per quanto riguarda i grafici:

\begin{enumerate}[noitemsep]
\item Grafico a scatter plot dei valori osservati della variabile target $(y)$ contro i regressori di $X$. Ovviamente bisognerà effettuare uno scatterplot per ogni variabile $X_i$;
\item Scatter plot dei valori predetti ($\hat{y}$) contro i residui stimati ($y - \hat{y} = \varepsilon$);
\item Scatter plot dei residui al quadrato ($\varepsilon^2$) contro i predetti ($\hat{y}$);
\item Scatter plot dei valori osservati ($y$) contro quelli predetti ($\hat{y}$);
\item Scatter plot dei residui ($\varepsilon$) contro i regressori di $X$. Ovviamente bisognerà effettuare un grafico per ogni regressore.
\end{enumerate}
Possiamo ricorrere anche ad alcuni test (metodo analitico):
\begin{itemize}
\item \textbf{Test di White}: questo test si basa sull’assunzione di omoschedasticità dei residui; viene perciò definita l’ipotesi nulla come $H0: Var(\varepsilon_i) = (\sigma^2)$ e l’ipotesi alternativa come $H1: Var(\varepsilon_i ) = ( \sigma^2_i )$.
Il test sfrutta la regressione $OLS$ del quadrato dei residui $\varepsilon_i^2$ sui regressori $x_j$, i regressori al quadrato $x_j^2$ e le loro interazioni.
Attraverso l'indice di determinazione $R^{2}$ di tale regressione, ricavato dal rapporto tra la variabilità spiegata dalla regressione ($SSE = \sum_{i}(\hat{y_i} - \bar{y})$) e la variabilità totale ($TSS = \sum_{i} (y_i - \bar{y})$), si calcola la statistica $LM=n R^{2}$ che si distribuisce come una $\chi^{2}$ con gradi di libertà uguale al numero di regressori. L’ipotesi nulla verrà rigettata se $LM$ risulterà maggiore del valore soglia della distribuzione $\chi^{2}$ (ovvero con p-value basso); %infatti se $R^{2}$ è oltre ad un certo valore significa che le variabili esplicative sono ancora significative nello spiegare i residui al quadrato e quindi che gli errori sono omoschedastici.
%@Dario: E` giusta l'ultima frase? Forse sono io che non capisco, oppure è di dubbia interpretazione: la parte "sono ancora significative nello spiegare..." vuole dire che effettivamente spiegano o il contrario?
%io avrei messo:
infatti se $R^{2}$ è oltre ad un certo valore significa che le variabili esplicative sono ancora significative nello spiegare la variabilità dei residui, ovvero che i residui (al quadrato) dipendono dai valori delle variabili esplicative $x_j$, come accade tipicamente in presenza di eteroschedasticità.

\item \textbf{Test di Breusch-Pagan}: anche in questo test l’ipotesi nulla è quella di omoschedasticità (H0: Var($\varepsilon_i$) = ($\sigma^2$)).
Si basa su una regressione di $\varepsilon^{2}_i$/$s^{2}$ dove $s^{2}$ è uguale alla sommatoria degli errori al quadrato diviso n. La somma dei quadrati dei regressori e quella degli scarti si distribuiscono come $\chi^{2}$ indipendenti. Il loro rapporto si distribuisce quindi come una $F$ di Snedecor. L’ipotesi nulla verrà rigettata quando la statistica $F$ è superiore ad un valore soglia.
Per risolvere tale problema si può procedere attraverso il metodo di stima $WLS$ (Weighted Least Squares).
\end{itemize}

\section{Errori autocorrelati}

Affinché il nostro modello lineare classico ottenga delle stime efficienti abbiamo bisogno di verificare se gli errori siano correlati tra di loro.
Accade spesso infatti, soprattutto in serie storiche o territoriali, che esista una correlazione tra errori in momenti successivi o territori vicini.
Gli errori correlati si possono scindere in due componenti: $\rho e_{ i-1}$ (errore ritardato di un tempo) e $\eta_i$ (errori omoschedastici Identicamente Distribuiti in modo normale). %FIXME: check notation
Si nota infatti che l’errore è legato al suo valore ritardato.
Possiamo classificare l’autocorrelazione in base al suo grado: si dice autocorrelazione di primo grado quando gli errori sono correlati con il loro valore ritardato di un tempo; allo stesso modo si dice autocorrelazione di i-esimo grado quando gli errori sono correlati con il loro valore ritardato di i gradi. Gli errori autocorrelati non incidono sulle proprietà di correttezza e consistenza degli stimatori OLS, ma solo sull’efficienza (non sono più BLUE). Come nel caso dell’eteroschedasticità la stima della varianza dei parametri e la relativa inferenza non sono più corretti e affidabili (la statistica T di Student ottiene dei valori erroneamente più elevati; gli intervalli di confidenza tendono ad essere più stretti e l'area di rifiuto del test anomalamente più ampia).
Per individuare la caratteristica di autocorrelazione si può ricorrere a rappresentazioni grafiche o a metodi analitici.

Per quanto riguarda i grafici:
\begin{enumerate}[noitemsep]
\item Scatter plot dei valori osservati ($y_i$) sui valori dei regressori ($x_j$). Ovviamente si costruiranno tanti scatter plot quanti regressori.
\item Scatter plot dei residui ($\varepsilon_i$) sulle variabili esplicative ($x_j$); ovviamente si costruiranno tanti scatter plot quanti regressori.
\item Scatter plot dei residui ($\varepsilon_i$) sui residui ritardati ($\varepsilon_{i-1}$) (a seconda del grado)
\item Correlogramma: in questo grafico vengono mostrate le correlazioni a diversi gradi con relativa barra di confidenza; analizzando acf (funzione di autocorrelazione dei residui) e pacf si riesce a determinare il tipo di modello autoregressivo.
\end{enumerate}

Test (metodi analitici):
\begin{itemize}
\item \textbf{Test di Durbin-Watson}: questo test si può effettuare a seconda dell’autocorrelazione dei diversi gradi. Prendiamo in esame il test per il primo grado.
Il test si basa sull’assunzione di non correlazione degli errori: l’ipotesi nulla è quindi $H0:  \rho = Corr( \varepsilon_i  ;  \varepsilon_{i-1} )=0$ contro l’ipotesi alternativa di autocorrelazione degli errori che può assumere i valori:
\begin{enumerate}[noitemsep]
\item Unidirezionale destra
\item Unidirezionale sinistra
\item Bidirezionale	
\end{enumerate}
%Check
La statistica $DW$ per l'autocorrelazione dei residui diventa uguale a $2(1-\rho)$. 
La distribuzione di $DW$ è centrata su $2$; se infatti $DW$ è uguale a $2$ gli errori sono incorrelati poichè $\rho = 0$; $DW$ tende a $0$ quando i residui sono correlati positivamente dato che $\rho = 1$; $DW$ tende a $4$ quando i residui sono correlati negativamente con $\rho = -1$. I valori critici cambiano di caso in caso ma convenzionalmente se non specificati sono $1$ e $3$. 

Nel caso in cui ci sia autocorrelazione lo stimatore più efficiente è il $GLS$. %Fix/check 

\end{itemize}

\section{Metodo di stima WLS; per soluzioni correlate, GLS }

\begin{itshape}
Errori eteroschedastici e incorrelati: modello WLS
\end{itshape}%FIX: structure

Per errori eteroschedastici si intende quando la varianza dell’errore non rimane costante al variare del valore delle variabili esplicative, violando quindi una delle ipotesi della regressione lineare classica. Per tale motivo gli stimatori OLS non possono essere usati (in quanto non più efficienti); al contrario si possono utilizzare gli stimatori weighted least squares (WLS) che permettono di stimare un modello per la varianza degli errori condizionata ai regressori. 
Si tratta di definire le seguenti nuove variabili che danno luogo al modello trasformato dividendo ogni variabile contenuta nel modello di partenza per la radice di $h(i)$ (corrisponde alla varianza di $\varepsilon$*, ovvero l'errore eteroschedastico). 
Infatti si tratta di stimare i parametri del modello trasformato con il metodo OLS regredendo $y^*$ su $X^*B$ riportando così la varianza degli errori ad una costante ottenendo la forma $y^* = X^*B + \varepsilon$.
\newline
\newline
\begin{itshape}
Errori omoschedastici e correlati: modello GLS
\end{itshape}

Nel caso invece ci si trovi davanti ad errori autocorrelati come accade in serie storiche e territoriali è ragionevole ipotizzare che esista correlazione fra errori in momenti successivi o territori vicini. Si parla di autocorrelazione se al variare di $X$ c'è fluttuazione dei valori di $Y$ con lo stesso segno (autocorrelazione \textit{positiva}), o segno alternato (autocorrelazione \textit{negativa}) oltre un certo intervallo di confidenza. Si possono ricavare stime per errori correlati in modo più semplice tramite una stima dei parametri in una equazione che tenga conto della struttura di autocorrelazione seriale (metodo proposto da Durbin). Bisogna stimare il coefficiente di autocorrelazione di primo ordine attraverso un modello avente come variabile risposta gli errori $\varepsilon_t^\sharp$ e come esplicative quelle già considerate più l’errore ritardato di un tempo $\varepsilon_{t-1}^\sharp$ e procedere alla stima del coefficiente di correlazione $\rho$. Una volta ottenuta la stima di $\rho$ si procede a moltiplicare ogni elemento dell'equazione ritardata per $\rho$ stesso: $\rho y_{t-1} = \rho\beta_0 + \rho\beta_1 x_{t-1} +\rho\varepsilon_{t-1}^\sharp$. Infine si procede a sottrarre l'equazione ritardata moltiplicata per $\rho$ all'equazione nella forma normale $y_t - \rho y_{t-1}$ ottenendo un modello OLS per i parametri trasformati $y_t^\sharp = \beta_0^\sharp + \beta_1 x_t^\sharp + w_i$ che rispetta tutte le classiche proprietà di correttezza, consistenza ed efficienza. In alternativa è possibile utilizzare un modello \textit{autoregressivo} (proprio del software SAS) per inserire nell’equazione iniziale un errore ritardato che tenga conto dell’autocorrelazione di ordine 1 (o anche ordini superiori): 
\begin{equation*}
y_i=b_0 +b_1 x_i +AR1_i  + \varepsilon_i. 
\end{equation*}
con $AR1 + \varepsilon_i = v_i$ e $Corr(v_i;v_j) = 0$.
\newline
\newline
\begin{itshape}
Errori eteroschedastici e correlati: stimatore GLS
\end{itshape}

Nel caso in cui gli errori non siano sferici in quanto eteroschedastici e correlati si utilizzano gli stimatori dei minimi quadrati generalizzati ($GLS$) interpretabili in modo analogo al modello classico in quanto stimatori $OLS$ basati su variabili trasformate per mezzo delle proprietà degli autovettori e autovalori ricavati dalla matrice dei residui $\Sigma_\varepsilon$. Lo stimatore risulta godere delle tre proprietà:
\begin{enumerate}[noitemsep]
\item Correttezza
\item Consistenza
\item Efficienza in quanto il teorema di Aitken stabilisce che nella classe degli stimatori lineari per il modello di regressione generalizzato lo stimatore $GLS$ è caratterizzato dalla minima varianza $\sigma^2 (X'^{\circ}X^{\circ})^{-1} >\sigma^2 (X'X)^{-1}$
\item Lo stimatore assegna un peso maggiore alle osservazioni caratterizzate da una minore varianza da considerarsi più ``affidabili''.
\end{enumerate}
Tutto questo è possibile assumendo come nota la matrice di varianze e covarianze dei residui $\Sigma_\varepsilon$. Nel caso in cui questa non fosse conosciuta allora è possibile ricorrere alla matrice campionaria $S_\varepsilon$ in modo che rispetti la condizione $\lim_{n\to \infty}{S_\varepsilon = \Sigma_\varepsilon}$. A questo punto si possono utilizzare gli stimatori FGLS (Feasible Generalized Least Squares)

\section{Multicollinearità }

Se la matrice $(X^{\prime} X)^{-1}$ non è invertibile oppure ha determinante prossimo allo 0 le stime non esistono (coefficienti sotto identificati, poiché non si dispone di sufficiente informazione per stimarli) o non sono stabili (coefficienti empiricamente sotto identificati).
Tale problema si verifica quando almeno una delle variabili è correlata linearmente alle altre e quindi si ha multicollinearità. In questo caso la matrice $(X^{\prime} X)$ è singolare e le soluzioni non sono uniche. 

Esistono due tipi di collinearità:
\begin{enumerate}[noitemsep]
\item \textbf{Perfetta}: sussiste quando almeno una variabile esplicativa è una combinazione lineare perfetta delle altre. Essa viola le proprietà del modello lineare classico solitamente per un errore nella definizione dei regressori o per una stranezza nei dati.
\item \textbf{Imperfetta}: sussiste quando $2$ o più regressori sono fortemente correlati e il determinante della matrice dei coefficienti tende a $0$.
\end{enumerate}
Le principali conseguenze sono:
\begin{enumerate}[noitemsep]
\item Un aumento della varianza delle stime dei coefficienti.
\item Gli intervalli di confidenza al cui interno sta il valore vero del parametro con probabilità $1-\alpha$ risultano essere più grandi di quanto non siano in realtà mentre la regione di accettazione del test si amplia notevolmente, ciò implica che i parametri vengano ritenuti non significativi anche quando in realtà lo sarebbero.
\item Con due variabili fortemente correlate se aggiungo la seconda, l’incremento di $R^{2}$ è inferiore all’incremento che avrei aggiungendo una seconda variabile debolmente correlata con la prima. Quindi, siccome le due variabili hanno molta varianza in comune, non posso dire quale delle due è più influente rispetto all’outcome.
\end{enumerate}
Esistono tre metodi analitici per verificare la presenza di multicollinearità:
\begin{enumerate}[noitemsep]
\item \textbf{Indice di tolleranza} che misura il grado di interrelazione di una variabile indipendente rispetto alle altre. Nella pratica, $TOL = 1 - R^{2}_j$ dove $R^{2}_j$ è calcolato dalla regressione della variabile esplicativa $X_j$ (usata come risposta) in funzione di tutte le altre esplicative. Per questo può assumera valori compresi tra $0$ (che indica la massima \textit{collinearità}) e $1$ (che indica la massima \textit{indipendenza} tra le variabili).

\item \textbf{Varianza multifattoriale} o \textbf{VIF}, ovvero il reciproco della tolleranza. 
Valori di tale indice variano tra $0$ e $\infty$ perciò se superiori a $20$ indicano uno stretto rapporto tra la variabile considerata e le altre ovvero un eccessivo grado di \textit{multicollinearità}. Vanno considerate con attenzione anche quelle variabili con valori di VIF maggiori di $10$.

\item \textbf{L'indice di condizione} è dato dalla radice del rapporto tra l’autovalore massimo della matrice $(X^{\prime}X)$ e ogni autovalore. Quando risulta essere maggiore di $30$ si ritiene esistere \textit{collinearità}. Tale convinzione viene rafforzata se un autovalore con condition index maggiore di $30$ contribuisce a spiegare elevate quote di varianza di due o più variabili.
\end{enumerate}

\section{Linearità}

L'approssimazione lineare non è sempre la migliore. La relazione ipotizzata tra la nostra variabile dipendente $y$ e le singole variabili esplicative $x$ è di tipo lineare: $y = f (x)$. %con f lineare però. Fixme 

Per validare la presenza di ciascun regressore all’interno dei diversi modelli dobbiamo quindi verificare la linearità di tale relazione.
Dunque, la variabile risposta deve essere una combinazione lineare di variabili esplicative e di parametri lineari.
Se una relazione tra $y$ e $X$ è non lineare l’effetto su $y$ di una variazione in $X$ dipende dal valore di $X$ poiché l’effetto marginale di $X$ non è costante. %non capisco bene il senso, questo è vero a prescindere dalla linearità.
Se una regressione lineare è mal specificata la forma funzionale è
errata e lo stimatore dell’effetto su $y$ di $X$ non è corretto nemmeno sulla media. %still not clear
Può capitare ad esempio che $R^{2}$ sia elevato ma che non ci sia linearità perchè c'è sia una componente lineare sia una non lineare. 
Per verificare la presenza (o meno) di linearità è possibile ricorrere ad alcuni grafici: 
\begin{enumerate}[noitemsep]
\item Scatter plot della risposta ($y_i$) in funzione di ogni esplicativa ($x_j$) presente nel modello.
\item Scatter plot dei residui ($\varepsilon_i$) in funzione dei valori osservati ($y_i$) della variabile dipendente; non deve essere un andamento sistematico.
\item Scatter plot dei residui ($\varepsilon_i$) in funzione dei valori previsti ($\hat{y_i}$); deve esserci un andamento regolare.
\end{enumerate}
È da notare che la non linearità potrebbe dipendere anche solo da una o da alcune variabili esplicative e non necessariamente da tutte.

Nel caso di non linearità dei parametri, potrebbe esistere una trasformazione che li renda lineari (caso linearizzabile) oppure no (intrinsecamente non lineari).

Nel primo caso si procede innanzitutto alla sostituzione del parametro o della variabile \textit{non lineare} con uno \textit{lineare}, poi si procede alla stima OLS ed infine si effettua l'operazione inversa riportando il parametro o la variabile alla forma originale. 

Nel caso invece di componenti intrinsecamente non lineari si procede allora alla stima attraverso gli stimatori NLS (minimi quadrati non lineari) che sfruttano algoritmi numerici nei software per affrontare il problema di minimizzazione non lineare.

Volendo utilizzare funzioni di variabili indipendenti non lineari in X possiamo riformulare una vasta famiglia di funzioni di regressione lineare come regressioni multiple.

Tra le funzioni non lineari le più utilizzate sono le polinomiali e le trasformazioni logaritmiche. 

Queste ultime in particolare possono essere applicate alle variabili esplicative, a quella dipendente o ad entrambe le tipologie. 

Esistono tre modelli principali: %fix ovunque
\begin{enumerate}[noitemsep]
\item \textbf{Linear-log}, in cui ad un incremento percentuale della variabile indipendente corrisponde un incremento nominale beta della variabile dipendente.
\item \textbf{Log-linear}, in cui ad un incremento nominale dell’esplicativa corrisponde un incremento percentuale beta della risposta.
\item \textbf{Log-log}, in cui entrambi gli incrementi sono percentuali.
\end{enumerate}

\section{Non normalità}

Quando gli errori $e_i$ sono indipendenti e identicamente distribuiti come $N(0,\sigma^{2})$ si possono ricavare la distribuzione degli stimatori, i test statistici, gli intervalli di confidenza e le proprietà ottimali (stima di massima verosimiglianza coincide con stima dei minimi quadrati). 
Nel caso in cui gli errori non siano normali, se tuttavia i campioni sono sufficientemente larghi per il teorema del limite centrale la distribuzione degli errori tende asintoticamente alla normalità. Se ciò non accade non è possibile applicare test e intervalli di confidenza perchè essi sono basati tutti sull’ipotesi di normalità degli errori.
%till here @Dario

Conseguenze della violazione della normalità:
\begin{enumerate}[noitemsep]
\item I parametri b possono essere espressi come combinazione lineare degli errori, per cui se gli errori non sono normali anch’essi non sono più normali
\item Non è più possibile ricavare test basati sulla normale standardizzata
\item Non è più possibile ricavare intervalli di confidenza per i parametri basati sulla normale standardizzata
\item Le stime OLS non coincidono con le stime ML ottenute attraverso il metodo della massima verosimiglianza, quindi gli stimatori OLS non sono più gli stimatori corretti a minima varianza fra tutti gli stimatori corretti e quindi non sono più BLUE
\end{enumerate}
Per individuare casi di non normalità è opportuno osservare indici quali media, mediana, varianza, simmetria e curtosi. Come rappresentazioni grafiche proposte abbiamo i box-plot, distribuzione dei residui, distribuzione cumulata dei residui, p-p plot (confronta i valori attesi associati alle probabilità cumulate dei residui campionari con i valori attesi associati alle corrispondenti probabilità cumulate teoriche della distribuzione normale) e q-q plot (confronta la distribuzione empirica di una variabile con una distribuzione normale). 
Esistono inoltre alcuni test non parametrici che non si basano su ipotesi sulla distribuzione. Per questo motivo sono molto utili per analizzare problemi di normalità dei residui.
\begin{enumerate}[noitemsep]
\item Test di Shapiro-Wilk, è compreso tra 0 e 1 e gli estremi corrispondono rispettivamente al rifiuto e all’accettazione dell’ipotesi di normalità
\item Test di Kolmogorov Smirnov, in cui l’ipotesi nulla è la normalità
\item Skewness test (test di asimmetria), ovvero un test direzionale basato sul fatto che la distribuzione della normale è simmetrica; si basa perciò su un indice di simmetria; rigettando H0 si rigetta la normalità, non rigettandola si dice solo che la distribuzione è simmetrica, ma non per forza normale
\item Test della Kurtosis (come per la simmetria, solo che si utilizza il momento quarto al posto del momento terzo)
\end{enumerate}
I problemi di non normalità possono essere risolti usando una trasformazione della variabile Y. La trasformazione può migliorare la relazione lineare tra la variabile dipendente e le variabili indipendenti.

\section{Outlier}

\begin{enumerate}[noitemsep]
\item Valori anomali (outlier): valori che si discostano in modo rilevante dall’andamento generale.
\item Punti influenti: punti che influenzano in misura rilevante le stime.
\end{enumerate}
Non sempre un valore anomalo è anche influente; per contro esistono punti non anomali che influiscono in misura rilevante sul risultato.

Come identificare gli outlier:
\begin{itemize}[noitemsep]
\item Rappresentazione grafica con Boxplot e Scatter plot.
\item Indicatori
	\begin{itemize}[noitemsep]
	\item Leverage: serve a verificare l’impatto che un caso specifico ha sulla capacità del modello di predire tutti i casi. Valori piccoli indicano che lo stimatore (b) di Y è basato sul contributo di molte osservazioni, mentre valori vicini a 1 indicano al contrario.
	\item Residui standardizzati: nel caso in cui il valore del residuo standardizzato sia maggiore di 3 probabilmente l’osservazione è un outlier.
	\item Residui studentizzati: sono utilizzati per verificare la presenza di osservazioni anomale in campioni di non elevata numerosità.
	\item Covariatio: indica la variazione nel determinante della matrice delle covarianze delle stime eliminando la i-esima osservazione.
	\item Dfitts: misura l’influenza di quel caso sulla stima dei coefficienti di regressione e sulla loro varianza, quando rimosso dal processo di stima. Male valori elevati.
	\item Dfbetas: misura l’influenza di quel caso, quando viene rimosso dal processo di stima, sulle stime di ogni coefficiente di regressione separatamente. Male valori elevati.
	\item Distanza di Cook: misura l’influenza di un singolo caso sulla stima dei coefficienti di regressione, in termini di capacità del modello di predire tutti i casi quando il singolo caso viene rimosso dal processo di stima. Valori superiori a 1 indicano che il punto è influente.
	\end{itemize}
\end{itemize}

\section{Modello lineare classico multivariato}

Consideriamo l’estensione multivariata (con più di una variabile dipendente) della regressione lineare multipla (con più di un regressore) che modella la relazione fra un insieme di r variabili esplicative z1,..,zr, e m variabili dipendenti y1,..,ym. Ognuna delle m variabili dipendenti è legata a una particolare regressione multipla. 

Ipotesi del modello:
\begin{enumerate}[noitemsep]
\item Parametri lineari.
\item Valori attesi degli errori casuali sono nulli.
\item Gli errori casuali all’interno gli ogni equazione e anche tra diverse equazioni sono omoschedastici e incorrelati pp più.
\item Le Z variabili esplicative sono non stocastiche: per ogni osservazione, il valore delle Z è una costante mentre il corrispondente valore di ogni y è una variabile casuale influenzata dagli errori casuali.
\item Le Z variabili esplicative sono non collineari con rango (z=r+1).
\item n $>$ r+1 per la stessa ragione, perciò per ogni equazione le stime dei minimi quadrati di b sono trovate in modo analogo al caso univariato.
\item Gli errori si distribuiscono come una normale multivariata.
\end{enumerate}
L’$R^{2}$ è una media pesata degli $R^{2}$ delle singole equazioni

\begin{remark}[] \ \\
\begin{itemize}
\item La dipendenza della variabile dipendente $y_j$ da Z non influenza la dipendenza della variabile $y_m$.
\item Abbiamo le stesse variabili esplicative in tutte le equazioni del sistema.
\item La correlazione simultanea tra i disturbi è costante nel tempo.
\end{itemize}
\end{remark}
\section{Inferenza nella Regressione Multivariata}

Guardare Modello lineare classico multivariato.
\newline
Gli stimatori OLS sono corretti ed efficienti. 
\newline
Si definisce VARIANZA GENERALIZZATA DI H il determinante della matrice di varianza-covarianza spiegata della popolazione (H) che si dimostra essere distribuita in modo indipendente dalla matrice degli errori.
\newline
Si definisce VARIANZA GENERALIZZATA DI SIGMA il determinante della matrice di varianza-covarianza residua (SIGMA) che si dimostra essere distribuita in modo indipendente dalla matrice degli errori.
\newline
Si può quindi definire come test del rapporto di verosimiglianza Lambda di Wilks: 
$\wedge$=I$\Sigma^{I}$/I$\Sigma^{I}$+$H^{I}$. Da $\wedge$ si ricava (1-$\wedge$)/$\wedge$, una distribuzione asintotica F (ovvero (1-$\wedge$)/$\wedge$ si distribuisce come una F).
\newline
Il test del rapporto di verosimiglianza per l’ipotesi H0: B\string^ = 0 è dato dal rapporto di varianze generalizzate Lambda di Wilks: $\wedge$=I$\Sigma^{I}$/I$\Sigma^{I}$+$H^{I}$. con H\string^ , $\Sigma$\string^ matrici di varianze covarianze spiegate e residue.
\newline
Se è vera H0 il numeratore e il denominatore di $\wedge$ tendono a coincidere poichè H\string^ tenderà a 0. Perciò la regione di accettazione di H0 (nullità dei parametri B\string^) è per valori di $\wedge$ vicini all’1.
\newline
La regione di rifiuto di H0 è per valori di $\wedge$ più piccoli di 1, in cui il numeratore è più piccolo del denominatore per la presenza di H\string^.
Data la struttura della F basata sul rapporto (1-$\wedge$)/ $\wedge$, al crescere di $\wedge$ decresce il numeratore e cresce il denominatore.
Perciò accetto H0 per valori di F superiori ad una certa soglia e rifiuto H0 per valori di F inferiori ad una certa soglia.
\newline
Perciò per il test basato su F asintotica:
regione di accettazione di H0 è per p value superiori ad $\alpha$;
regione di rifiuto di H0 per p value inferiori ad $\alpha$;
Analogamente si costruiscono intervalli di confidenza per i parametri e per i valori predetti delle Y.
\newline
Altri test si costruiscono in modo analogo:
\begin{itemize}[noitemsep]
\item Test sulla non significatività di un gruppo di variabili esplicative rispetto a tutte le variabili dipendenti.
\item Test sull’uguaglianza dei parametri relativi a diversi gruppi di variabili esplicative nelle singole equazioni.
\item Test sull'uguaglianza dei parametri relativi alle stesse variabili in coppie di diverse equazioni.
\end{itemize}


\section{Modello lineare generalizzato}

Guardare Modello lineare classico multivariato.
\newline
Quando cambiano le ipotesi sugli errori si ha il modello lineare generalizzato: Y = BZ+E in cui la matrice di covarianza degli errori non è più necessariamente diagonale e gli errori potrebbero essere eteroschedastici. 
\newline
\newline
\newline
\newline
\newline

Nell’ipotesi classica:

\begin{enumerate}[noitemsep]
\item Gli errori sono omoschedastici all’interno delle stesse equazioni: per ogni individuo rispetto alla medesima variabile dipendente la parte spiegata è uguale
\item Gli errori sono omoschedastici tra equazioni diverse: per ogni individuo rispetto alle diverse variabili dipendenti la parte spiegata è uguale
\item Gli errori sono incorrelati all’interno delle stesse equazioni: il comportamento di ogni individuo rispetto alla medesima variabile dipendente non è legato a quello degli altri individui
\item Gli errori sono incorrelati fra equazioni diverse: il comportamento di ogni individuo rispetto a diverse variabili dipendenti non è legato al proprio e a quello degli altri individui.
\newline
\newline
Nell’ipotesi intermedia:
\newline
\item Gli errori sono omoschedastici all’interno delle stesse equazioni: per ogni individuo rispetto alla medesima variabile dipendente la parte spiegata è uguale
\item Gli errori sono eteroschedastici tra equazioni diverse: per ogni individuo rispetto alle diverse variabili dipendenti la parte spiegata è diversa
\item Gli errori sono incorrelati all’interno delle stesse equazioni: il comportamento di ogni individuo rispetto alla medesima variabile dipendente non è legato a quello degli altri individui
\item Gli errori sono correlati fra equazioni diverse: il comportamento di ogni individuo rispetto a diverse variabili dipendenti è legato al proprio e a quello degli altri individui.
\newline
\newline
Nell’ipotesi estrema:
\newline
\item Gli errori sono eteroschedastici all’interno delle stesse equazioni: per ogni individuo rispetto alla medesima variabile dipendente la parte spiegata è diversa
\item Gli errori sono eteroschedastici tra equazioni diverse: per ogni individuo rispetto alle diverse variabili dipendenti la parte spiegata è diversa
\item Gli errori sono correlati all’interno delle stesse equazioni: il comportamento di ogni individuo rispetto alla medesima variabile dipendente è legato a quello degli altri individui
\item Gli errori sono correlati fra equazioni diverse: il comportamento di ogni individuo rispetto a diverse variabili dipendenti è legato al proprio e a quello degli altri individui.
\end{enumerate}
Quindi occorre usare non le singoli sottomatrici di correlazione degli errori $\Sigma\varepsilon(i)$, ma la matrice $\Sigma\varepsilon$ relativa all’intero modello.

\section{Modello SURE}

Secondo un approccio più realistico, degli r regressori si usano solo i regressori effettivamente legati alle diverse variabili dipendenti: r1 nella prima, r2 nella seconda, ..., rm nell’ultima.
\newline
In altre parole nel Modello SURE abbiamo regressori diversi per ogni equazione all’interno dell’insieme complessivo dei regressori per l’insieme delle equazioni del modello. 
\newline
$\Sigma$jrj è quindi la somma di tutti i regressori nelle diverse equazioni.
\newline
Ciò permette di risolvere anche il problema di una numerosità diversa delle osservazioni delle diverse equazioni. Data nj è la numerosità delle osservazioni per l’equazione j-esima il complesso delle numerosità è dato da $\Sigma$jnj.
\newline
\newline
La soluzione dei minimi quadrati per la stima dei coefficienti sembra simile a quella dei minimi quadrati generalizzati ma solo in apparenza: 
\begin{itemize}
\item Il modello è caratterizzato dalla presenza delle variabili esplicative ZA, ZB, ZC diverse da equazione ed equazione.
\item Gli errori sono:
	\begin{itemize}[noitemsep]
	\item omoschedastici e incorrelati nella stessa equazione
	\item eteroschedastici fra diverse equazioni
	\item correlati per lo stesso individuo e incorrelati tra individui diversi fra diverse equazioni
	\end{itemize}
\end{itemize}

\section{Il problema dei dati gerarchici e uso di Regressione multilevel}

I modelli statistici si basano su campionamento casuale semplice da popolazione infinita o finita con reinserimento. In tal caso vige l’ipotesi di indipendenza tra le singole osservazioni.
\newline 
In molti casi, però i dati risultano essere raggruppati in cluster ovvero presentano una struttura gerarchica (ad esempio Ospedale - Pazienti). In tali casi il campionamento casuale semplice non risulta efficiente, ma appare preferibile effettuare un campionamento a più stadi perché si desidera analizzare le relazioni tra le variabili che possono essere misurate a livelli di raggruppamento dei dati diversi (livelli gerarchici della struttura dei dati).
\newline
Il campionamento a stadi implica la dipendenza tra le osservazioni appartenenti allo stesso gruppo (esse hanno medesima probabilità di essere estratte).
\newline
Ad esempio gli studenti appartenenti alla stessa scuole condividono stesso ambiente, stessi insegnanti, stesso quartiere di provenienza oltre a scambi e comunicazioni tra essi.
\newline
La dipendenza tra le unità di primo livello (micro) appartenenti alla stessa unità di secondo livello (macro) è cruciale per l’analisi.
\newline
\newline
Cosa succede se si ignora la struttura gerarchica dei dati?
\begin{enumerate}[noitemsep]
\item FALLACIA ECOLOGICA: quando ad una variabile riferita al livello macro si vuole dare validità micro.
\item FALLACIA ATOMISTICA: quando ad una variabile riferita al livello micro si vuole dare validità macro.
\end{enumerate}
la variabile dipendente y ha sia un aspetto individuale sia di gruppo; la variabile x pur essendo misurata a livello individuale contiene anche una quota di variabilità imputabile al gruppo, infatti la media di x in un gruppo può essere diversa dalla media di x in un altro gruppo poiché la composizione della x nei gruppi può essere diversa.
\newline
Le regressioni a livello macro considerano i dati aggregati dati dalla media di x ed y e sono quindi diverse dalle regressioni a livello micro tra x ed y.
\newline
L’analisi delle relazioni entro i gruppi può portare a risultati molto diversi da quelli ottenuti considerando le relazioni tra i gruppi.
In altri termini la struttura dei dati ed il loro raggruppamento può avere effetto anche in altro modo: facendo variare i coefficienti della regressione da gruppo a gruppo.
\newline
\newline
Yij = $\beta$0j + $\beta$ijxij + rij
\newline
\newline
Diverse intercette $\beta$0j e coefficienti di regressione $\beta$1j per ciascun gruppo:
\begin{itemize}	
\item se i coefficienti $\beta$0j e $\beta$1j sono entrambe costanti allora la struttura
gerarchica non ha effetto = regressione OLS;
\item se i due coefficienti dipendono entrambe da j allora la regressione OLS
non può essere utilizzata:
	\begin{itemize}
	\item Se varia solo $\beta$0j con j allora si ha un modello random intercept
 	\item Se anche $\beta$1j varia con j allora il modello è detto random coefficient
	\end{itemize}
\end{itemize}
Pertanto data la struttura dei dati, si può pensare di porre assieme la regressione tra i gruppi e la regressione entro i gruppi.


\section{Modello multilevel: definizione e significato}

Il Modello Multilevel è un Modello di analisi della covarianza a effetti casuali, la regressione cattura la relazione disaggregata tra i dati e quindi descrive la varianza nei gruppi, mentre l’analisi della varianza cattura la relazione aggregata fra i gruppi e quindi descrive la varianza fra gruppi. Spezza in due l’analisi mettendo insieme la covarianza: elimina gli aspetti individuali, analizza gli effetti di gruppo e riesce ad attribuire la varianza al gruppo di appartenenza.
\newline
In un primo tipo di modelli (mixed models) la relazione disaggregata tra i dati e la varianza nei gruppi sono descritte mediante parametri fissi mentre la relazione aggregata fra i gruppi
e la varianza fra gruppi sono descritte come variabili casuali.
\newline
In un secondo tipo di modelli (random models) anche la relazione disaggregata tra i dati e la varianza nei gruppi sono descritte come variabili casuali.
\newline
\newline
I modelli finora studiati possono essere visti come sottocasi del modello Multilevel:
\begin{enumerate}
\item Per $u_j=0$ e nessuna gerarchia dei dati, Modello Lineare: 
\begin{equation*}
y_i= \gamma_{00}+\Sigma_k \beta_k(x_{ik}-\bar{x}_k) + \varepsilon_i \ ;
\end{equation*}
\item Per $u_j=0$ Regressione Multilevel: 
\begin{equation*}
y_{ij} = \gamma_{00} + \Sigma_k \beta_k(x_{ijk}-\bar{x}_k) + \varepsilon_{ij};
\end{equation*}
\item Per $\Sigma_k \beta_k (x{ik} - \bar{x}_k) = 0$ e $u_j$ fisso Analisi Varianza: 
\begin{equation*}
y_{ij} = \gamma_{00}+ u_j + \varepsilon_{ij};
\end{equation*}
\item Per $\Sigma_k\beta_k(x_{ik}- \bar{x}_k) = 0$ e $u_j$ stocastico Analisi Varianza Casuale: 
\begin{equation*}
y_{ij} = \gamma_{00} + u_j + \varepsilon_{ij}.
\end{equation*}
\end{enumerate}

\section{Modello Multilevel: OLS, Empty, Mixed, Total Effects}

La stima del modello multilevel si compone di 4 step:
\begin{enumerate}[noitemsep]
\item Si stima innanzitutto il modello lineare solitamente con il metodo di stima OLS (Unstructured ordinary least squares model (OLS)).
\item Poi si propone l’empty model (Unconditional means model UMM) vale a dire l’analisi della varianza a effetti casuali.
\item Random intercepts model (RIM) cioè l’analisi della covarianza a effetti casuali per l’analisi della varianza.
\item Random slopes and intercepts model (UGM) analisi della covarianza a effetti casuali sia per il modello lineare che per l’analisi della varianza.
\end{enumerate}
OLS: 
Si consideri un modello Multilevel in cui appare solo la parte del Modello Lineare che viene stimata mediante metodo OLS con una sola variabile e ipotizzando che le variabili X e Y siano centrate ($y_{ij}= \beta_0+\Sigma_{jk} \beta_k x_{ijk}+\varepsilon_{ij}$).
\newline
In questo modo si vede quale sia l’effetto delle variabili esplicative sulla variabile dipendente se i dati non fossero centrati. Naturalmente gli errori si distribuiscono come una normale. 
\newline
\newline
EMPTY MODEL:
Nel modello ANOVA ad effetti casuali detto anche empty model si ha che: $y_{ij}=v_j+r_{ij}$.
In questo caso la variabile dipendente y dipende dagli effetti casuali:
\begin{itemize}
\item a livello di gruppo, Vj, distribuiti in modo normale $N(\gamma_0, \tau_2)$
\item a livello individuale, dai residui $R_{ij}$, distribuiti in modo normale $N(0, \sigma_2)$ 
\end{itemize}
La variabilità all’interno di ogni gruppo è quindi dovuta solamente alla distribuzione casuale della variabile dipendente.
\newline
L’intercetta casuale a livello di gruppo può essere scomposta in due parti: l’intercetta fissa media tra tutti i gruppi e la misura della sua deviazione attorno alla media tra i gruppi di tipo casuale: $v_j=\gamma_0+u_j$.
\newline
Possiamo riscrivere il modello nel seguente modo: $y_{ij}=\gamma_0+u_{jj}+r_{ij}$.
\newline
In questo modello quindi la variabilità totale di y può essere scomposta nella somma delle varianze ai due livelli, varianza fra i gruppi e varianza nei gruppi: $var(y)=var(U_j)+var(R_{ij})=\sigma_2+\tau_2$.
\newline
Si può quindi definire il coefficiente $r^{2} / (r^{2}+\sigma^{2})$  di  correlazione intraclasse: 
\newline
Il coefficiente di correlazione intraclasse misura quindi la quota di varianza di y spiegata dall’appartenenza ai gruppi degli Individui. Se il coefficiente di correlazione intraclasse è nullo, ovvero tutti gli uj sono nulli, allora il raggruppamento è irrilevante ed è inutile utilizzare altri modelli rispetto alla regressione semplice. Se invece il coefficiente di correlazione intraclasse è positivo è necessario considerare un modello gerarchico.
\newline
Il test F come in ogni analisi della varianza può essere utilizzato per verificare in termini inferenziali l’ipotesi che le intercette casuali uj siano nel complesso tra loro equivalenti (se non c’è differenza fra gruppi). In questo caso il test F serve per capire se nel complesso vale l’ipotesi nulla che le medie parziali ottenute nel campione possano essere ritenute nel complesso equivalenti. Per confrontare tra loro le strutture di secondo livello (ad esempio scuole, ospedali, università) come nell’analisi della varianza casuale non si utilizzano i valori delle medie campionarie, non informative del vero valore di Uj ma i loro intervalli di confidenza che comprendono con una probabilità del 90\%, 95\%, 99\% i valori veri ignoti di Uj.
\newline
Ciò significa probabilizzare la gerarchia fra medie parziali in quanto, quanto più sono piccoli gli intervalli di confidenza è maggiore la loro capacità di fornire informazioni sui valori veri ignoti di Uj. A differenza che nell’analisi della varianza casuale nel Modello Multilevel che è come ricordato un’analisi della covarianza casuale, tali intervalli di confidenza sono al netto dell’influenza delle variabili X del modello lineare. Questa probabilizzazione della gerarchia influenza e rende più robusto il confronto fra strutture di secondo livello in quanto una media parziale uj di una struttura J si considera superiore a un’altra media parziale ug di una struttura G se e solo se l’estremo inferiore del suo intervallo di confidenza inf (uj) è più grande dell’estremo superiore dell’altra sup(ug) in quanto solo in questo caso con un elevato grado di probabilità il valore vero di J sarà più grande di G.

MIXED MODEL: %Fix structure

Se si inserisce nel modello una variabile esplicativa $x_k$ il modello diventa il vero e proprio random intercept model (mixed model) $y_{ij}=\gamma_0+\beta_1 x_{ij}+ u_j + \delta_{ij}$ dove $u_j$ è la determinazione della variabile casuale $U_j \ (j=1,...,p)$ variabili casuali indipendenti e identicamente distribuite normalmente dalla formula $N(\gamma_0, \tau_2)$ e rappresentano i residui di 2 livello. Esse sono indipendenti e quindi incorrelate con i residui di primo livello $\delta_{ij}$ determinazioni delle variabili casuali normalmente distribuite $\Delta_{ij} \sim N(0,\sigma_2)$

In questo caso la variabile dipendente y dipende:
\begin{itemize}
\item dalle variabili x e dai relativi parametri fissi $\beta_1$.
\item dall’effetto casuale a livello di gruppo, $u_j$, che si distribuisce in modo normale  $N(\gamma_0, \tau_2)$.
\item dall’effetto casuale a livello individuale $\delta_{ij}$, che si distribuisce in modo normale $N(0,\sigma_2)$.
\end{itemize}
La correlazione intraclasse misura la quota di varianza di y spiegata dall’appartenenza ai gruppi degli Individui al netto della quota di varianza spiegata da x (a differenza del modello empty in questa circostanza ho x che spiega una parte della variabilità non dovuta all’appartenenza a un gruppo di un individuo). Per questa ragione il suo valore può decrescere anche molto dal caso empty. Il modello comprende 4 parametri da stimare:
\begin{itemize}
\item i coefficienti di regressione $\gamma_0$ e $\beta_1$ e le componenti della varianza $\sigma^{2}$ e $\tau^{2}_0$.
\item Il coefficiente di regressione $\beta_1$ può essere interpretato come variazione di Y corrispondente ad una variazione unitaria di x.
\item In un modello di regressione semplice la variabilità di Y non spiegata dalla regressione è semplicemente data dal residui $\delta_{ij}$.
\end{itemize}
La variabilità in un modello multilevel fa riferimento a più popolazioni:
\begin{itemize}
\item Le v.c. $U_j$ possono essere viste come le variabili casuali che descrivono i residui a livello di gruppo, ovvero gli effetti di gruppo non spiegati da x.
\item La v.c. $\Delta$ può essere vista come variabile casuale che descrive i residui a livello individuale, ovvero gli effetti individuali non spiegati da x.
\end{itemize}
Rappresentazioni di un modello random intercept:
\begin{itemize}
\item Micro model: $y_{ij}=\beta_1 x_{ij}+R_{ij}$
\item Macro model: $\beta_{0j}=\gamma_{00}+U_{0j}$
\end{itemize}
Come unica equazione multilevel: $y_{ij}=\gamma_{00}+\beta_1 x_{ij}+ U_{0j}+R_{ij}$
\begin{itemize}
\item parte fissa del modello $y_{ij}=\gamma_{00}+\beta_1 x_{ij}$
\item parte casuale (random part) del modello $U_{0j}+ R_{ij}$
\end{itemize}
Come varianze e covarianze:
\begin{itemize}
\item livello 1 $\sigma^{2}$
\item livello 2 $\tau^{2}_0$
\newline
\end{itemize}
TOTAL MODEL: 
\newline
La relazione tra variabile dipendente ed esplicative può variare tra i gruppi in modi diversi: si può avere un’eterogeneità delle regressioni tra i diversi gruppi (si parla anche di interazione gruppo – covariate). Ad esempio nel caso dell’analisi delle performance degli studenti appartenenti alle scuole, si può assumere che l’effetto dello stato socio economico o dell’intelligenza individuale sulle performance possa essere diverso nelle singole scuole.
\newline
La struttura dei dati ed il loro raggruppamento può essere spiegato quindi anche facendo variare i coefficienti della regressione da gruppo a gruppo.
\newline
$y_{ij}=\beta_{0j}+\beta_{1j} x_{ij}+ R_{ij}$
\begin{itemize}
\item diversi $\beta_{0j}$ (INTERCETTE)
\item diversi $\beta_{1j}$ (COEFFICIENTI DI REGRESSIONE: l’effetto di X su Y può essere diverso nei singoli gruppi)
\item se i coefficienti $\beta_{0j}$ e $\beta_{1j}$ sono entrambi costanti e la struttura gerarchica non ha effetto: regressione OL
\item se solo il coefficiente dell’intercetta $\beta_{0j}$ varia con j allora si ha un modello random intercept
\item mentre se anche il coefficiente di regressione $\beta_{1j}$ varia con j allora il modello è detto random coefficient
\end{itemize}

\section{Metodi di stima e Verifica di ipotesi}

I parametri da stimare nel modello random intercept sono: coefficienti di regressione $\gamma$ e componenti di varianza, $\sigma_2$ e $\tau_2$. Gli effetti casuali $U_{0j}$ non sono parametri ma variabili casuali latenti, ovvero non direttamente osservabili. La letteratura riporta metodi per la stima dei parametri sotto l’assunzione che i residui $U_{0j}$ e $R_{ij}$ siano distribuiti normalmente quali: ML e REML. Esistono due metodi di stima (sotto l’assunzione di normalità dei residui) per la stima dei parametri, il full maximum likelihood (ML) e il restricted (anche chiamato residual) maximum likelihoo (REML).
\newline
REML è alternativo a ML. Questo metodo massimizza la verosimiglianza (likelihood) dei residui osservati ottenendo le stime degli effetti fissi usando metodi «non likelihoodlike» come ordinary least squares (OLS) o generalized least squares (GLS)) e successivamente usa queste per massimizzare la verosimiglianza dei residui (sottraendo gli effetti misti) per ottenere le stime dei parametri della varianza. Diversi algoritmi sono disponibili per ottenere queste stime: EM (expectation-maximisation), Fischer Scoring, IGLS e RIGLS. Questi sono algoritmi iterativi che convergono dopo alcune iterazioni alle stime ML o REML.
Per testare i parametri fissi del modello si utilizza la seguente ipotesi nulla (ipotesi di significatività) su ciascun parametro $\rightarrow H0: \gamma_h = 0$.
\newline
Questa ipotesi viene verificata con un test t; questo test è noto come WALD TEST. Sotto l’ipotesi nulla il test ha approssimativamente una distribuzione t con d.f. basati sulla struttura multilevel dell’analisi. Per testare più parametri (fissi e random) del modello invece viene utilizzato il deviance test. Dalla stima del modello lineare con il metodo ML si ottiene la verosimiglianza del modello, da cui: DEVIANCE = -2 lnL (misura della bontà di adattamento ai dati del modello).
\newline
Solitamente la deviance viene interpretata in termini differenziali, ovvero si calcola la differenza tra le deviance di modelli alternativi. Si tratta di confrontare i valori osservati della variabile dipendente con i valori teorici di due modelli: 
\begin{enumerate}[noitemsep]
\item con le variabili esplicative di interesse e l’altro senza alcuna variabile (empty-model); 
\item con le variabili esplicative di interesse e l’altro che contiene “tanti parametri quante sono le osservazioni” (saturated model).
\end{enumerate}
Il confronto si basa sulla funzione di log-verosimiglianza. Perciò indicate rispettivamente con D0, Dmod, Dsat le devianze calcolate per il modello vuoto (empty-model), il modello considerato e il modello saturo, valori di Dmod più prossimi a 0 che non a D0 faranno propendere per ritenere “buono” il modello considerato. Ognuna delle devianze ha distribuzione asintotica Chi-quadrato (con j gradi di libertà pari al numero delle esplicative). Le loro differenze avrà distribuzione Chi-quadrato (con k gradi di libertà pari alla differenza del numero di esplicative).
\newline
Se l’obiettivo è quello di sottoporre a verifica l’ipotesi che riguarda la nullità congiunta di tutti i coefficienti (esclusa l’intercetta), si può pensare a ragion veduta di operare un confronto fra due modelli, l’empty-model e il modello ipotizzato. Questo test può essere applicato sia alla parte fissa sia a quella random del modello.


\end{document}